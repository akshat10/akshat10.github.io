<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Akshat Mishra | Reinforcement Learning with Deep-Q Learning</title>
  <meta name="description" content="Training a neural network to play the Atari standard Arcade Games using only the visual frame buffer as an input.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Reinforcement Learning with Deep-Q Learning">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/posts/reinforcement-learning">
  <meta property="og:description" content="Training a neural network to play the Atari standard Arcade Games using only the visual frame buffer as an input.">
  <meta property="og:site_name" content="Akshat Mishra">
  <meta property="og:image" content="http://localhost:4000/assets/og-image.jpg">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://localhost:4000/posts/reinforcement-learning">
  <meta name="twitter:title" content="Reinforcement Learning with Deep-Q Learning">
  <meta name="twitter:description" content="Training a neural network to play the Atari standard Arcade Games using only the visual frame buffer as an input.">
  <meta name="twitter:image" content="http://localhost:4000/assets/og-image.jpg">
  <link rel="shortcut icon" href="favicon.ico" />

  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">

  <link type="text/css" rel="stylesheet" href="/assets/light.css">

</head>

<body>
  <main role="main">
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav reveal">
  <a href="/" style="display:inline-block;font-family:"Cormorant Garamond", serif;font-size:36px;font-weight:900;color:#000;text-decoration:none" class="header-logo" title="Akshat Mishra">Akshat Mishra</a>
  <ul class="header-links">
    
      <li>
        <a href="/about" title="About me">
          <span class="icon icon-android-person"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://in.linkedin.com/in/akshat10
" target="_blank" title="LinkedIn">
          <span class="icon icon-social-linkedin"></span>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.instagram.com/akshat.10/" target="_blank" title="Instagram">
          <span class="icon icon-social-instagram"></span>
        </a>
      </li>
    
    
    
    
    
      <li>
        <a href="mailto:contact@akshat.me" target="_blank" title="Email">
          <span class="icon icon-at"></span>
        </a>
      </li>
    
    
  </ul>
</nav>

        <article class="article reveal">
          <header class="article-header">
            <h1>Reinforcement Learning with Deep-Q Learning</h1>
            <p>Training a neural network to play the Atari standard Arcade Games using only the visual frame buffer as an input.</p>
            <div class="article-list-footer">
              <span class="article-list-date">
                January, 2016
              </span>
<!--
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
                
                
                  2 minute read
                
              </span>
-->
<!--
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                  <a href="/tag/ai">ai</a>
                
                  <a href="/tag/videogame">videogame</a>
                
              </div>
-->
            </div>
          </header>

          <div class="article-content">
            <p>In the past, reinforcement learning models have required the programmer to explicitly define the features to define a state space. Recent breakthroughs in deep reinforcement learning however have obviated the need for precisely defining a feature vector. This allows the models to learn feature representations optimal to the environment. This automated approach often results in better features than would be found in explicit feature representations.</p>

<p>Using basic raw pixel stream as input, an agent was successfully trained to learn control policies in an Atari game environment. The fundamental model is a convolutional neural network trained with a variation of Q-learning. The Atari game environment for running simulations and training the agent is created using the OpenAI Gym platform.</p>

<h2 id="motivation">Motivation</h2>
<p>The end objective of this project was to train a neural network to play the Atari standard Arcade Games using only the visual frame buffer as an input to our model. Hence, the agent would learn to play broadly in the same way as humans, with the same visual cues and automatically identify which actions have positive/negative impacts.</p>

<h2 id="procedure">Procedure</h2>
<p>The simulations used in this project were an implementation of the famous Atari Games series on virtual environment based on the open-sourced environment called <a href="https://gym.openai.com/">Gym</a> by OpenAI. The environment by OpenAI makes is easiy for researchers to test their algorithms in environments without having to go through the hassle of creating one themselves. For this project, Ms Pacman was used and two convolutional layers were constructed along a fully connected neural network classifier to predict actions based on the current state space.</p>

<figure>
  <a href="/assets/documentation/dqn/mspacman.jpg" class="fluidbox-trigger">
    <img src="/assets/documentation/dqn/mspacman.jpg" alt="Ms Pacman" style="margin:0 auto;" /> 
  </a>
  <figcaption style="text-align: center;">Ms Pacman game</figcaption>
</figure>

<p><br /></p>
<h2 id="observations-and-results">Observations and Results</h2>
<p>The behavior of the agent was analysed during and after training as it learned to navigate the environment and avoid obstacles to achieve maximum reward. There have been certain instances where the agent shows peculiar behavior, especially when it got stuck at a local minima for the loss function. One reason for this could be that even though the agent is rewarded a score of 0.0 in such situations, the frequency of ghost attacks in corners is less than that of agent being near the center of the environment.</p>

<figure>
  <a href="/assets/documentation/dqn/minima.PNG" class="fluidbox-trigger">
    <img src="/assets/documentation/dqn/minima.PNG" alt="Ms Pacman" style="margin:0 auto;" /> 
  </a>
  <figcaption style="text-align: center;">Agent stuck near minima</figcaption>
</figure>

<p>Another interesting behavior from the agent is seen when it attains sudden high scores during training. This is because the agent learns that colliding with the ghosts, when it gets a special block of collectible point, can destroy the ghost and fetch extra points. This property of the environment was discovered by the agent due to the random exploration feature embedded in the algorithm by the parameter.</p>

<figure>
  <a href="/assets/documentation/dqn/goodscore.PNG" class="fluidbox-trigger">
    <img src="/assets/documentation/dqn/goodscore.PNG" alt="Ms Pacman" style="margin:0 auto;" /> 
  </a>
  <figcaption style="text-align: center;">Agent achieves a good score</figcaption>
</figure>


          </div>

<!--
          <div class="article-share">
            
            <a href="" title="Share on Twitter" onclick="window.open('https://twitter.com/home?status=Reinforcement Learning with Deep-Q Learning - http://localhost:4000/posts/reinforcement-learning ', 'newwindow', 'width=500, height=225'); return false;">
              <span class="icon icon-social-twitter"></span>
            </a>
            <a href="" title="Share on Facebook" onclick="window.open('https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/reinforcement-learning', 'newwindow', 'width=500, height=500'); return false;">
              <span class="icon icon-social-facebook"></span>
            </a>
            <a href="" title="Share on Google+" onclick="window.open('https://plus.google.com/share?url=http://localhost:4000/posts/reinforcement-learning', 'newwindow', 'width=550, height=400'); return false;">
              <span class="icon icon-social-googleplus"></span>
            </a>
          </div>
-->
          
          

          
        </article>
        <footer class="footer reveal">
  
  <p>
    <a href="#top">Back to top</a> -  
    <a href="/about" title="About me">About Me</a>
    
  </p>
  <p>
    Powered by Jekyll and theme by Nielsen Ramon.
  </p>
</footer>


      </div>
    </div>
  </main>
  <script type="text/javascript" src="/assets/vendor.js"></script>
<script type="text/javascript" src="/assets/application.js"></script>

<script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.16/webfont.js"></script>
<script>
  WebFont.load({
    google: {
      families: ['Cormorant Garamond:700', 'Lato:300,400,700']
    }
  });
</script>

<!--

-->

</body>
</html>
